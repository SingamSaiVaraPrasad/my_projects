{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtHlgrXJ8E4CDCSl9jngnc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SingamSaiVaraPrasad/my_projects/blob/main/DQN_frozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Xo--Gt1i-Ca",
        "outputId": "401b45ec-7d53-4579-c295-06b787107501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch as torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import nn.functional as F"
      ],
      "metadata": {
        "id": "ZypIvhWtjZEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class deepQnet(nn.Module):\n",
        "                                             # when ever we define a class we need to have __init__\n",
        "  def __init_(self,nInput,nHidden,nOutput):\n",
        "    super().__init__()\n",
        "    self.fc1=nn.Linear(nInput,nHidden)\n",
        "    self.outfcn=nn.Linear(nHidden,nOutput)\n",
        "  def forwardFcn(self,x_inp):                #this calculates output for an input through the network\n",
        "    self.x_inp=F.ReLU(self.fc1(x_inp))\n",
        "    x_inp=self.outfcn(x_inp)\n",
        "    return x_inp"
      ],
      "metadata": {
        "id": "TYyZpzNIuww7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class replay_buffer_memory():\n",
        "  def __init__(self,maxlen):\n",
        "    self.memory=deque([],maxlen=maxlen)   # double ended queue stores the states rewards nextstates actions\n",
        "  def addData(self,transition):\n",
        "    self.memory.append(transition)\n",
        "  def sample(self,sampSize):\n",
        "    return random.sample(self.memory,sampSize)     # gives random batch of given size\n",
        "  def __len__(self):\n",
        "    return len(self.memory)\n"
      ],
      "metadata": {
        "id": "-dgVjoL324oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class lake():\n",
        "  learning_rate=0.001              # alpha for the equation of Temporal difference\n",
        "  disc_factor=0.9                   #gamma\n",
        "  replay_mem=1000                   # memory of replay buffer\n",
        "  batch_sz=32                       # buffer size each time\n",
        "  sync_rate=10                      # time steps after which sync of target and policy networks sync\n",
        "\n",
        "  lossfcn=nn.MSEloss()\n",
        "  optimizer=none\n",
        "  ACTIONS=['L','R','U','D']\n",
        "  def train(self,episodes,render=False,is_slippery=False):\n",
        "    env=gym.make('FrozenLake-v1',map_name=\"4x4\",is_slippery=is_slipper,render_mode='human' if render else none)\n",
        "    num_states=env.observation_space.n\n",
        "    num_actions=env.action_space.n\n",
        "\n",
        "    epsilon=1 #100% random actions\n",
        "    memory=replay_buffer_memory(self.replay_mem) #creating an object of memory class for lake object\n",
        "\n",
        "    # create a target and policy network\n",
        "    policy_net=deepQnet(num_states,num_states,num_actions)\n",
        "    target_net=deepQnet(num_states,num_states,num_actions)\n",
        "    target_net.load_state_dict(policy_net.state_dict()) ##make target and policy networks same\n",
        "\n",
        "    self.optimizer=torch.optim.Adam(policy_net.parameters(),self.learning_rate)\n",
        "    rewards=np.zeros(episodes) # rewards are stored in episodes initialized to 0\n",
        "\n",
        "    #epsilon decay has to be tracked\n",
        "    epsi=[]\n",
        "    step_cnt=0;\n",
        "    for i in range(episodes):\n",
        "      env.reset()[0]\n",
        "      terminated=False\n",
        "      truncated=False\n",
        "      while((not terminated) and (not truncated) ):\n",
        "        if random.random()<epsilon:\n",
        "          action=env.action_space.sample()\n",
        "        else\n",
        "          with torch.no_grad:\n",
        "            action=policy_net(self.actions_to_dqn(state,num_states)).argmax().item()  #gives current state and total states to DQN\n",
        "        nxt_state,reward,truncated,terminated,_= env(action)\n",
        "        memory.addData(state,nxt_state,action,reward,terminated)\n",
        "        state=nxt_state\n",
        "        step_cnt+=1\n",
        "        if reward==1:\n",
        "          rewards[i]=1\n",
        "        if(len(memory)>batch_sz and np.sum(rewards)>0): # only if the batch size is met and rewards are received\n",
        "          mini_bactch=memory.sample(self.batch_sz)\n",
        "          self.optim(mini_batch,policy_net,target_net) #a function\n",
        "\n",
        "      def optim(self,mini_batch,policy_net,target_net):\n",
        "        current_q_list=[]\n",
        "        current_targetQlist=[]\n",
        "        for state,nxt_state,action,reward,terminated in mini_batch:\n",
        "          if terminated:\n",
        "            target=reward\n",
        "          else:\n",
        "            with torch.no_grad:\n",
        "                target=reward+disc_factor*target_net(self.actions_to_dqn(state,num_states)).max()\n",
        "          current_q=policy_net(self.actions_to_dqn(state,num_states))\n",
        "          current_q_list.append(current_q)\n",
        "          target_q=target_net(self.actions_to_dqn(state,num_states))\n",
        "          target_q[action]=target                        # for the current tupple, it has an action, for that action we are finding the bellman equation using above line\n",
        "          current_targetQlist.append(current_target_q)   #then we put them into list after finding new target for current action\n",
        "                                                         # this two lists are sent to update weights using back propogation\n",
        "        loss=self.(torch.stack(current_q_list),torch.stack(current_targetQlist))\n",
        "        self.optimizer.zero_grad()                       #initially gradient of all weights are made zero other wise they accumulate\n",
        "        loss.backward()                                  #calculates gradient and stores\n",
        "        self.optimizer.step()                            #updates the weights\n",
        "\n",
        "\n",
        "\n",
        "      def actions_to_dqn(self,stt,num_stts):\n",
        "        arr=tensor.zeros(num_stts)\n",
        "        arr[stt]=1\n",
        "        return arr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y9BAQEHm8EBr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}